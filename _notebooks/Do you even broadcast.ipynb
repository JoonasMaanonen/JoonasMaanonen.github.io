{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "355e1f24-bd93-42f0-afeb-f159f05fbfad",
   "metadata": {},
   "source": [
    "# Do you even broadcast?\n",
    "PyTorch and Numpy have this amazing magic trick that allows you to make your code super fast called [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html). I have used it many times before without even realising but I never took the time to properly learn the fundamentals of it. The purpose of this blog post is to serve as a basic introduction to the topic and for me to finally properly learn how broadcasting works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cfac20-f8ed-46b6-9cae-642a0c62acd0",
   "metadata": {},
   "source": [
    "## What is broadcasting?\n",
    "\n",
    "Broadcasting defines what happens when you do arithmetic operations on two tensors or numpy arrays with different shapes. Generally when you do broadcasted operations those are done in efficient C-code meaning that they will be much faster than regular Python array operations. \n",
    "\n",
    "That's enough of general jargon, broadcasting is best understood through examples. From now on I will use PyTorch tensors for illustrating the different broadcasting operations, but the principles also work for Numpy arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8313c49f-14c8-4b08-9950-1ac9804b44d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a790f485-8778-48e9-b685-282696e46e59",
   "metadata": {},
   "source": [
    "Let's first create two dummy tensors that we use for experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3fbe559-f28d-4ab0-9dd4-760e53563b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa59999d-3318-4e0a-8787-093239c702b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.Size([3]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e6058d-3019-4d1c-955f-a3ba06c587bd",
   "metadata": {},
   "source": [
    "Generally it is intuitive for us what happens when two tensor with same dimensions are multiplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6d3fedb-3945-47a3-83dc-4933fcaed96a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca025ead-677d-4bce-bc11-3b5b76b1db59",
   "metadata": {},
   "source": [
    "Corresponding elements just multiply each other.\n",
    "\n",
    "\n",
    "But what happens when the dimensions do not match? This is where broadcasting rules come to play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53a99e2b-932e-412a-86ca-8db8a7576827",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb3c3517-c14e-4f4a-af61-37e5e67a4d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  4,  9],\n",
       "        [ 4, 10, 18]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z * x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2681d2-538b-40f7-84eb-e7f6c9c6b3a1",
   "metadata": {},
   "source": [
    "Hmm, what happened here? Let's first try to deduce it from the result. \n",
    "\n",
    "It looks like that *x* multiplied each row of *z* elementwise.\n",
    "\n",
    "This is what broadcasting is essentially, the smaller tensor somehow deduced how to apply itself over the larger tensor. \n",
    "\n",
    "Luckily there are simple rules that make this work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2718dabc-2889-42b4-9471-602d139c22bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([3]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d510c39d-9047-44ef-840f-477ffc2bd6bc",
   "metadata": {},
   "source": [
    "Let's now introduce the two rules that define what tensors can be broadcast together.\n",
    "\n",
    "The following rules are from the PyTorch broadcasting [documentation](https://pytorch.org/docs/stable/notes/broadcasting.html). I suggest you check that out as well. \n",
    "\n",
    "Two tensors can be broadcast if:\n",
    "  - Each tensor has at least one dimension.\n",
    "  - When iterating the two tensors over the dimensions starting from the trailing dimension (right) the following rules must apply:\n",
    "    - The dimensions are equal between the two tensors\n",
    "    - One of the dimensions is 1\n",
    "    - One of the dimensions is missing (PyTorch will make the axis 1)\n",
    "    \n",
    "Let's now go through examples on how this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aadc4287-0a4c-43fb-b89f-42039e70df42",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3,4,5)\n",
    "y = torch.ones(3,4,5)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5e8ee92-9a7b-4c39-9953-f468b59b0cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4, 5]), torch.Size([3, 4, 5]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6485d55c-adfa-4e78-8d10-fd88ad8a6b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x * y "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa97c81f-2fee-484c-97ab-1ae4305c517f",
   "metadata": {},
   "source": [
    "The following operation works because all the dimensions match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bb0c45e-3fee-4431-b9bb-1ab79dbc0308",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(3,4)\n",
    "y = torch.ones(3,4,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bac999-fc3d-4b4c-8c85-9a7ff7a725ce",
   "metadata": {},
   "source": [
    "What do you think will happen here if we were to try arithmetic operations on these two tensors?\n",
    "\n",
    "Stop and think for a minute or two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e70ac75d-1805-4fb8-9e63-ad5a9be34ba7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_933/4205270810.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (5) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcd44a0-6aac-4453-a161-fff6bda04b7c",
   "metadata": {},
   "source": [
    "We get an error that indicates that the tensor sizes do not match.\n",
    "\n",
    "Why was this? Remember the tensor broadcasting rules? You start comparing the dimensions from the trailing dimensions, so the rightmost dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60a61b6c-70d4-4ea8-b6e9-0d6217c19c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 4]), torch.Size([3, 4, 5]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8bf2a9-6eda-4a71-9440-5e0a60879ac2",
   "metadata": {},
   "source": [
    "So in this case we would compare 4 and 5 and realise that the numbers are not equal nor one of them is not one. \n",
    "\n",
    "This means that the broadcasting rules are violated and so PyTorch cannot operate with these two shapes\n",
    "\n",
    "How can we try fixing this?\n",
    "\n",
    "One thing that comes to mind is inserting one as the rightmost dimension of tensor x. That would make the dimensions match nicely. \n",
    "\n",
    "The question is how can we do this in PyTorch. After some Googling there are basically two ways:\n",
    "  - x.unsquueze()\n",
    "  - x[:, :, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a73bc48b-3a64-4948-83c5-3178ee000d80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 1])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.unsqueeze(dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "083a0d0e-53d6-4f4e-b040-a66da0fd23d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 1])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, :, None].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
